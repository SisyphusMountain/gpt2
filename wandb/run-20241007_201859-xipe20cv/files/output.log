2024-10-07 20:19:00,435 - INFO - Enabling compilation
2024-10-07 20:19:01,698 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-07 20:19:02,548 - INFO - total desired batch size: 524288
2024-10-07 20:19:02,548 - INFO - => computed gradient accumulation steps 32
2024-10-07 20:19:02,549 - INFO - Starting epoch 0
W1007 20:19:24.818000 139429471421632 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-07 20:19:25,010 - INFO - tokens per second = 23342 | loss: 10.936690 | gradient norm 0.1740 | dt 22460.955620 ms | CUDA memory: 526.96 MB | CPU memory: 1.93 GB
2024-10-07 20:19:29,424 - INFO - tokens per second = 118905 | loss: 10.918173 | gradient norm 0.1927 | dt 4409.300804 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:33,685 - INFO - tokens per second = 123040 | loss: 10.879021 | gradient norm 0.1717 | dt 4261.102438 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:37,936 - INFO - tokens per second = 123324 | loss: 10.822794 | gradient norm 0.1681 | dt 4251.289606 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:42,050 - INFO - tokens per second = 127426 | loss: 10.755556 | gradient norm 0.1762 | dt 4114.447355 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:46,211 - INFO - tokens per second = 125995 | loss: 10.671740 | gradient norm 0.1662 | dt 4161.185026 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:50,379 - INFO - tokens per second = 125801 | loss: 10.580461 | gradient norm 0.1495 | dt 4167.595387 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:54,623 - INFO - tokens per second = 123541 | loss: 10.499744 | gradient norm 0.1369 | dt 4243.839979 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:19:59,015 - INFO - tokens per second = 119379 | loss: 10.403515 | gradient norm 0.1223 | dt 4391.783237 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:20:03,375 - INFO - tokens per second = 120271 | loss: 10.327673 | gradient norm 0.1160 | dt 4359.212637 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 313, in <module>
    logging.info(f"tokens per second = {tokens_per_second:.0f} | loss: {total_loss.item():.6f} | "
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 2216, in info
    root.info(msg, *args, **kwargs)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1539, in info
    self._log(INFO, msg, args, **kwargs)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1684, in _log
    self.handle(record)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1700, in handle
    self.callHandlers(record)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1762, in callHandlers
    hdlr.handle(record)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1028, in handle
    self.emit(record)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/wandb/sdk/lib/redirect.py", line 645, in write
    self._old_write(data)
KeyboardInterrupt
