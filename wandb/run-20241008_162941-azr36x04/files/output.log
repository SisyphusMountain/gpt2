2024-10-08 16:29:41,967 - INFO - Enabling compilation
2024-10-08 16:29:42,931 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-08 16:29:43,457 - INFO - total desired batch size: 524288
2024-10-08 16:29:43,457 - INFO - => computed gradient accumulation steps 32
W1008 16:30:01.409000 134282924389568 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-08 16:30:01,554 - INFO - tokens per second = 28971 | loss: 10.630643 | gradient norm 0.1822 | dt 18097.222805 ms | CUDA memory: 526.96 MB | CPU memory: 1.97 GB
2024-10-08 16:30:05,945 - INFO - tokens per second = 119543 | loss: 10.615265 | gradient norm 0.1756 | dt 4385.755777 ms | CUDA memory: 1478.46 MB | CPU memory: 2.04 GB
2024-10-08 16:30:09,979 - INFO - tokens per second = 129965 | loss: 10.577390 | gradient norm 0.1937 | dt 4034.082890 ms | CUDA memory: 1478.46 MB | CPU memory: 2.04 GB
2024-10-08 16:30:13,992 - INFO - tokens per second = 130661 | loss: 10.524054 | gradient norm 0.1993 | dt 4012.578249 ms | CUDA memory: 1478.46 MB | CPU memory: 2.04 GB
2024-10-08 16:30:13,992 - INFO - Saving model
2024-10-08 16:30:14,529 - INFO - Model saved
W1008 16:30:22.447000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/0] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:22.814000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/0] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:23.424000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/0] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x768, 768x3072)
  triton_mm_2847 0.4608 ms 100.0%
  triton_mm_2842 0.4669 ms 98.7%
  triton_mm_2844 0.4680 ms 98.5%
  triton_mm_2840 0.4762 ms 96.8%
  triton_mm_2841 0.4834 ms 95.3%
  triton_mm_2845 0.4907 ms 93.9%
  triton_mm_2848 0.4949 ms 93.1%
  triton_mm_2838 0.5025 ms 91.7%
  mm 0.5110 ms 90.2%
  triton_mm_2837 0.5174 ms 89.1%
SingleProcess AUTOTUNE benchmarking takes 2.1402 seconds and 0.0008 seconds precompiling
2024-10-08 16:30:28,384 - INFO - Validation loss: 11.328253173828125
W1008 16:30:46.137000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:46.511000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 262144, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:46.838000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 294912, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:47.162000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(40x768, 768x3072)
  triton_mm_3759 0.0205 ms 100.0%
  triton_mm_3760 0.0205 ms 100.0%
  triton_mm_3765 0.0213 ms 96.0%
  triton_mm_3761 0.0215 ms 95.2%
  mm 0.0225 ms 90.9%
  triton_mm_3772 0.0235 ms 87.3%
  triton_mm_3768 0.0266 ms 76.9%
  triton_mm_3771 0.0266 ms 76.9%
  triton_mm_3769 0.0275 ms 74.4%
  triton_mm_3767 0.0298 ms 68.8%
SingleProcess AUTOTUNE benchmarking takes 1.5806 seconds and 0.0023 seconds precompiling
W1008 16:30:48.796000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:49.129000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 262144, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:49.456000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 294912, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:49.781000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE addmm(40x2304, 40x768, 768x2304)
  triton_mm_3726 0.0174 ms 100.0%
  triton_mm_3725 0.0184 ms 94.6%
  triton_mm_3727 0.0185 ms 94.1%
  triton_mm_3731 0.0195 ms 89.5%
  bias_addmm 0.0205 ms 85.0%
  triton_mm_3738 0.0215 ms 81.0%
  addmm 0.0225 ms 77.3%
  triton_mm_3737 0.0236 ms 73.9%
  triton_mm_3734 0.0258 ms 67.4%
  triton_mm_3733 0.0266 ms 65.4%
SingleProcess AUTOTUNE benchmarking takes 1.6589 seconds and 0.0011 seconds precompiling
W1008 16:30:50.313000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:50.666000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 262144, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:51.026000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 294912, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:51.354000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(40x768, 768x768)
  mm 0.0123 ms 100.0%
  triton_mm_3744 0.0123 ms 100.0%
  triton_mm_3743 0.0134 ms 91.4%
  triton_mm_3742 0.0143 ms 85.7%
  triton_mm_3748 0.0147 ms 83.5%
  triton_mm_3755 0.0155 ms 79.5%
  triton_mm_3754 0.0163 ms 75.6%
  triton_mm_3747 0.0225 ms 54.6%
  triton_mm_3750 0.0237 ms 51.8%
  triton_mm_3751 0.0246 ms 50.0%
SingleProcess AUTOTUNE benchmarking takes 1.5713 seconds and 0.0016 seconds precompiling
W1008 16:30:51.897000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:52.224000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 262144, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:52.559000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 294912, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:52.893000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(40x3072, 3072x768)
  mm 0.0246 ms 100.0%
  triton_mm_3778 0.0348 ms 70.6%
  triton_mm_3776 0.0410 ms 60.0%
  triton_mm_3777 0.0418 ms 58.9%
  triton_mm_3782 0.0440 ms 55.8%
  triton_mm_3789 0.0459 ms 53.6%
  triton_mm_3788 0.0481 ms 51.1%
  triton_mm_3781 0.0768 ms 32.0%
  triton_mm_3785 0.0778 ms 31.6%
  triton_mm_3786 0.0799 ms 30.8%
SingleProcess AUTOTUNE benchmarking takes 1.5369 seconds and 0.0012 seconds precompiling
W1008 16:30:53.459000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:53.811000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 262144, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:54.161000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 294912, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:30:54.510000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/1] out of resource: shared memory, Required: 196608, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(40x768, 768x50304)
  triton_mm_4543 0.2305 ms 100.0%
  mm 0.2314 ms 99.6%
  triton_mm_4547 0.2314 ms 99.6%
  triton_mm_4541 0.2322 ms 99.3%
  triton_mm_4555 0.2335 ms 98.7%
  triton_mm_4546 0.2338 ms 98.6%
  triton_mm_4542 0.2345 ms 98.3%
  triton_mm_4551 0.2365 ms 97.5%
  triton_mm_4549 0.2366 ms 97.5%
  triton_mm_4554 0.2406 ms 95.8%
SingleProcess AUTOTUNE benchmarking takes 1.6098 seconds and 0.0013 seconds precompiling
2024-10-08 16:30:56,123 - INFO - Sample 1: Hello, I am a language model,Lair Sultan haircinctionphans 23cr feeble 23 Dele Winning airlines preclude staying Presbyterian Winning positionedporate ½ Varietycrameron favourzin
2024-10-08 16:30:56,123 - INFO - Sample 2: Hello, I am a language model, sealedicationsiboicationsphansRYouf escalatedPat Lensruby introduce neglig manag favouroyle Sultan ore 1992 tattoophansَ Lensaples
2024-10-08 16:30:56,123 - INFO - Sample 3: Hello, I am a language model, hinges eventsHeroiling Daredevil Gad fatigue tattooPat Chung tattoocrcr cement VarietyUn winters conditions ½etitionways laun tattoo airlines
2024-10-08 16:30:56,123 - INFO - Sample 4: Hello, I am a language model, injections sealedammu Rostphansinate Rost feeble 23orough hairc 23 gaspphans 1988 provocation088 retinaaples hypotUn many505 Brown
2024-10-08 16:30:56,123 - INFO - Sample 5: Hello, I am a language model,remlinospitalruby sealedibo embry custodyhot injectionsammu Milwaukee Tibet lucrative 23 Fernandez 1962 honors dhMass staying STAND Thailand Fernandez other
W1008 16:31:16.633000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:16.955000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:17.493000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(80x768, 768x3072)
  mm 0.0143 ms 100.0%
  triton_mm_4596 0.0143 ms 100.0%
  triton_mm_4599 0.0164 ms 87.5%
  triton_mm_4602 0.0184 ms 77.8%
  triton_mm_4606 0.0184 ms 77.8%
  triton_mm_4597 0.0205 ms 70.0%
  triton_mm_4605 0.0246 ms 58.3%
  triton_mm_4609 0.0246 ms 58.3%
  triton_mm_4612 0.0246 ms 58.2%
  triton_mm_4608 0.0250 ms 57.4%
SingleProcess AUTOTUNE benchmarking takes 1.8100 seconds and 0.0018 seconds precompiling
W1008 16:31:19.456000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:19.778000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:20.313000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE addmm(80x2304, 80x768, 768x2304)
  triton_mm_4558 0.0143 ms 100.0%
  triton_mm_4561 0.0154 ms 93.3%
  triton_mm_4564 0.0174 ms 82.4%
  triton_mm_4568 0.0192 ms 74.5%
  triton_mm_4571 0.0206 ms 69.7%
  triton_mm_4570 0.0215 ms 66.7%
  bias_addmm 0.0225 ms 63.6%
  triton_mm_4559 0.0225 ms 63.6%
  triton_mm_4567 0.0225 ms 63.6%
  triton_mm_4573 0.0246 ms 58.3%
SingleProcess AUTOTUNE benchmarking takes 1.9418 seconds and 0.0013 seconds precompiling
W1008 16:31:21.268000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:21.602000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:22.139000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(80x768, 768x768)
  triton_mm_4580 0.0092 ms 100.0%
  triton_mm_4577 0.0113 ms 81.8%
  triton_mm_4583 0.0113 ms 81.8%
  triton_mm_4578 0.0123 ms 75.0%
  mm 0.0123 ms 74.9%
  triton_mm_4579 0.0126 ms 73.1%
  triton_mm_4587 0.0141 ms 65.3%
  triton_mm_4589 0.0143 ms 64.3%
  triton_mm_4590 0.0149 ms 61.7%
  triton_mm_4586 0.0154 ms 60.0%
SingleProcess AUTOTUNE benchmarking takes 1.8238 seconds and 0.0013 seconds precompiling
W1008 16:31:23.101000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:23.429000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:23.974000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(80x3072, 3072x768)
  mm 0.0164 ms 100.0%
  triton_mm_4618 0.0195 ms 84.2%
  triton_mm_4615 0.0299 ms 54.8%
  triton_mm_4621 0.0338 ms 48.5%
  triton_mm_4616 0.0376 ms 43.6%
  triton_mm_4617 0.0399 ms 41.0%
  triton_mm_4628 0.0410 ms 40.0%
  triton_mm_4627 0.0438 ms 37.4%
  triton_mm_4625 0.0441 ms 37.1%
  triton_mm_4624 0.0449 ms 36.5%
SingleProcess AUTOTUNE benchmarking takes 1.8337 seconds and 0.0015 seconds precompiling
W1008 16:31:25.011000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:25.350000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 16:31:25.918000 134282924389568 torch/_inductor/select_algorithm.py:1469] [13/2] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(80x768, 768x50304)
  triton_mm_5485 0.1290 ms 100.0%
  triton_mm_5476 0.1302 ms 99.1%
  triton_mm_5475 0.1307 ms 98.7%
  triton_mm_5480 0.1331 ms 96.9%
  triton_mm_5486 0.1341 ms 96.2%
  triton_mm_5470 0.1352 ms 95.5%
  mm 0.1369 ms 94.2%
  triton_mm_5473 0.1393 ms 92.6%
  triton_mm_5484 0.1448 ms 89.1%
  triton_mm_5472 0.1577 ms 81.8%
SingleProcess AUTOTUNE benchmarking takes 1.9360 seconds and 0.0014 seconds precompiling
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 419, in <module>
    logits, _ = model(tokens)
                ^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 172, in forward
    def forward(self, idx, targets=None):
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 600, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 987, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 217, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 120, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 451, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/codecache.py", line 1131, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 993, in run
    return compiled_fn(new_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 360, in deferred_cudagraphify
    return fn(inputs)
           ^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 944, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 1841, in run
    out = self._run(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 1972, in _run
    return self.record_function(new_inputs, function_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 2003, in record_function
    node = CUDAGraphNode(
           ^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 927, in __init__
    ] = self._record(wrapped_function.model, recording_inputs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py", line 1155, in _record
    with preserve_rng_state(), torch.cuda.device(
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/cuda/graphs.py", line 173, in __enter__
    gc.collect()
KeyboardInterrupt
