2024-10-08 13:39:28,795 - WARNING - Disabling compilation
2024-10-08 13:39:29,737 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-08 13:39:30,328 - INFO - total desired batch size: 524288
2024-10-08 13:39:30,328 - INFO - => computed gradient accumulation steps 32
2024-10-08 13:39:30,328 - INFO - Starting epoch 0
2024-10-08 13:39:35,483 - INFO - tokens per second = 101793 | loss: 11.002542 | gradient norm 0.1692 | dt 5150.540113 ms | CUDA memory: 1022.55 MB | CPU memory: 1.95 GB
2024-10-08 13:39:40,534 - INFO - tokens per second = 103807 | loss: 10.982696 | gradient norm 0.1883 | dt 5050.582647 ms | CUDA memory: 1973.05 MB | CPU memory: 2.02 GB
2024-10-08 13:39:45,479 - INFO - tokens per second = 106017 | loss: 10.948652 | gradient norm 0.1705 | dt 4945.330858 ms | CUDA memory: 1973.05 MB | CPU memory: 2.02 GB
2024-10-08 13:39:45,945 - INFO - Saving model
2024-10-08 13:39:46,247 - INFO - Model saved
2024-10-08 13:39:50,725 - INFO - tokens per second = 99937 | loss: 10.892008 | gradient norm 0.1655 | dt 5246.165991 ms | CUDA memory: 1973.05 MB | CPU memory: 2.03 GB
2024-10-08 13:39:55,828 - INFO - tokens per second = 102753 | loss: 10.830953 | gradient norm 0.1752 | dt 5102.393866 ms | CUDA memory: 1973.05 MB | CPU memory: 2.03 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 303, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 274, in training_step
    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 21, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 82, in clip_grad_norm_
    clip_coef = max_norm / (total_norm + 1e-6)
                ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_tensor.py", line 35, in wrapped
    @functools.wraps(f, assigned=assigned)

KeyboardInterrupt
