2024-10-07 20:16:49,253 - INFO - Enabling compilation
2024-10-07 20:16:50,525 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-07 20:16:51,348 - INFO - total desired batch size: 524288
2024-10-07 20:16:51,348 - INFO - => computed gradient accumulation steps 32
2024-10-07 20:16:51,349 - INFO - Starting epoch 0
W1007 20:17:13.602000 128779987125440 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-07 20:17:13,828 - INFO - tokens per second = 23323 | loss: 10.936153 | gradient norm 0.1679 | dt 22479.014874 ms | CUDA memory: 526.96 MB | CPU memory: 1.93 GB
2024-10-07 20:17:18,007 - INFO - tokens per second = 125589 | loss: 10.922645 | gradient norm 0.1836 | dt 4174.623251 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:22,111 - INFO - tokens per second = 127752 | loss: 10.890073 | gradient norm 0.1685 | dt 4103.952885 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:26,169 - INFO - tokens per second = 129198 | loss: 10.831939 | gradient norm 0.1592 | dt 4058.019638 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:30,230 - INFO - tokens per second = 129104 | loss: 10.771273 | gradient norm 0.1677 | dt 4060.965300 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:34,292 - INFO - tokens per second = 129053 | loss: 10.689555 | gradient norm 0.1650 | dt 4062.577248 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:38,356 - INFO - tokens per second = 129028 | loss: 10.601391 | gradient norm 0.1460 | dt 4063.380003 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:42,426 - INFO - tokens per second = 128807 | loss: 10.519708 | gradient norm 0.1356 | dt 4070.345163 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:46,498 - INFO - tokens per second = 128750 | loss: 10.428301 | gradient norm 0.1213 | dt 4072.142839 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:50,572 - INFO - tokens per second = 128703 | loss: 10.358442 | gradient norm 0.1157 | dt 4073.618412 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:54,647 - INFO - tokens per second = 128665 | loss: 10.277475 | gradient norm 0.1076 | dt 4074.826241 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:17:58,722 - INFO - tokens per second = 128670 | loss: 10.219105 | gradient norm 0.0995 | dt 4074.684620 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:03,132 - INFO - tokens per second = 118887 | loss: 10.161006 | gradient norm 0.0989 | dt 4409.964561 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:07,463 - INFO - tokens per second = 121050 | loss: 10.108644 | gradient norm 0.0948 | dt 4331.171036 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:11,626 - INFO - tokens per second = 125938 | loss: 10.028600 | gradient norm 0.0936 | dt 4163.064003 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:15,821 - INFO - tokens per second = 124958 | loss: 9.980411 | gradient norm 0.0901 | dt 4195.730448 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:19,898 - INFO - tokens per second = 128608 | loss: 9.937625 | gradient norm 0.1087 | dt 4076.645136 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:23,974 - INFO - tokens per second = 128633 | loss: 9.887370 | gradient norm 0.0808 | dt 4075.833797 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:28,051 - INFO - tokens per second = 128602 | loss: 9.894526 | gradient norm 0.0857 | dt 4076.828718 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:32,127 - INFO - tokens per second = 128626 | loss: 9.866747 | gradient norm 0.0753 | dt 4076.074600 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:36,203 - INFO - tokens per second = 128608 | loss: 9.769742 | gradient norm 0.0730 | dt 4076.639891 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:40,279 - INFO - tokens per second = 128631 | loss: 9.718292 | gradient norm 0.1317 | dt 4075.918198 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:44,636 - INFO - tokens per second = 120335 | loss: 9.700323 | gradient norm 0.0719 | dt 4356.917858 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:18:49,028 - INFO - tokens per second = 119387 | loss: 9.683075 | gradient norm 0.0671 | dt 4391.487598 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 302, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 265, in training_step
    @torch.compile(mode="max-autotune", disable=disable_compilation)
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 267, in torch_dynamo_resume_in_training_step_at_267
    optimizer.zero_grad()
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 271, in torch_dynamo_resume_in_training_step_at_271
    loss.backward()
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 21, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 82, in clip_grad_norm_
    clip_coef = max_norm / (total_norm + 1e-6)
                ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_tensor.py", line 35, in wrapped
    @functools.wraps(f, assigned=assigned)

KeyboardInterrupt
