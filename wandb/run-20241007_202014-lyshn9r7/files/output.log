2024-10-07 20:20:15,533 - INFO - Enabling compilation
2024-10-07 20:20:16,792 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-07 20:20:17,619 - INFO - total desired batch size: 524288
2024-10-07 20:20:17,619 - INFO - => computed gradient accumulation steps 32
2024-10-07 20:20:17,620 - INFO - Starting epoch 0
W1007 20:20:39.967000 125186372199616 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-07 20:20:40,157 - INFO - tokens per second = 23263 | loss: 11.020823 | gradient norm 0.1791 | dt 22537.215471 ms | CUDA memory: 526.96 MB | CPU memory: 1.93 GB
2024-10-07 20:20:44,222 - INFO - tokens per second = 129128 | loss: 11.003760 | gradient norm 0.1963 | dt 4060.230732 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:20:48,293 - INFO - tokens per second = 128771 | loss: 10.963129 | gradient norm 0.1712 | dt 4071.490765 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:20:52,372 - INFO - tokens per second = 128547 | loss: 10.905585 | gradient norm 0.1700 | dt 4078.569889 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:20:56,739 - INFO - tokens per second = 120060 | loss: 10.838076 | gradient norm 0.1801 | dt 4366.884232 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:01,093 - INFO - tokens per second = 120405 | loss: 10.752204 | gradient norm 0.1730 | dt 4354.364872 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:05,215 - INFO - tokens per second = 127190 | loss: 10.664876 | gradient norm 0.1536 | dt 4122.087955 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:09,364 - INFO - tokens per second = 126374 | loss: 10.575185 | gradient norm 0.1408 | dt 4148.713350 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:13,461 - INFO - tokens per second = 127973 | loss: 10.475707 | gradient norm 0.1239 | dt 4096.856594 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:17,660 - INFO - tokens per second = 124854 | loss: 10.401487 | gradient norm 0.1173 | dt 4199.222088 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:21,819 - INFO - tokens per second = 126057 | loss: 10.314599 | gradient norm 0.1126 | dt 4159.127474 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:26,082 - INFO - tokens per second = 123001 | loss: 10.252175 | gradient norm 0.1016 | dt 4262.480259 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:30,282 - INFO - tokens per second = 124811 | loss: 10.182249 | gradient norm 0.1025 | dt 4200.641155 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:34,355 - INFO - tokens per second = 128716 | loss: 10.125051 | gradient norm 0.0970 | dt 4073.220730 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:38,618 - INFO - tokens per second = 122993 | loss: 10.040002 | gradient norm 0.0936 | dt 4262.750864 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:42,927 - INFO - tokens per second = 121700 | loss: 9.989402 | gradient norm 0.0899 | dt 4308.038235 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:47,134 - INFO - tokens per second = 124599 | loss: 9.943611 | gradient norm 0.1070 | dt 4207.806110 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:51,317 - INFO - tokens per second = 125354 | loss: 9.894779 | gradient norm 0.0806 | dt 4182.472229 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:55,571 - INFO - tokens per second = 123243 | loss: 9.899787 | gradient norm 0.0836 | dt 4254.099846 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:21:59,857 - INFO - tokens per second = 122338 | loss: 9.872516 | gradient norm 0.0749 | dt 4285.553694 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:04,108 - INFO - tokens per second = 123300 | loss: 9.777281 | gradient norm 0.0721 | dt 4252.135515 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:08,320 - INFO - tokens per second = 124482 | loss: 9.724671 | gradient norm 0.1303 | dt 4211.748362 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:12,507 - INFO - tokens per second = 125238 | loss: 9.712469 | gradient norm 0.0722 | dt 4186.332226 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:16,803 - INFO - tokens per second = 122026 | loss: 9.691663 | gradient norm 0.0682 | dt 4296.541929 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:21,017 - INFO - tokens per second = 124401 | loss: 9.645053 | gradient norm 0.0688 | dt 4214.513302 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:25,124 - INFO - tokens per second = 127672 | loss: 9.626877 | gradient norm 0.0691 | dt 4106.518984 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
2024-10-07 20:22:29,351 - INFO - tokens per second = 124031 | loss: 9.606073 | gradient norm 0.0703 | dt 4227.070570 ms | CUDA memory: 1478.46 MB | CPU memory: 1.94 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 302, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 265, in training_step
    @torch.compile(mode="max-autotune", disable=disable_compilation)
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 267, in torch_dynamo_resume_in_training_step_at_267
    optimizer.zero_grad()
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 271, in torch_dynamo_resume_in_training_step_at_271
    loss.backward()
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1716, in __getattr__
    def __getattr__(self, name: str) -> Any:

KeyboardInterrupt
