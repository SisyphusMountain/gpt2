2024-10-08 13:30:49,582 - INFO - Enabling compilation
2024-10-08 13:30:50,596 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-08 13:30:51,482 - INFO - total desired batch size: 524288
2024-10-08 13:30:51,483 - INFO - => computed gradient accumulation steps 32
2024-10-08 13:30:51,484 - INFO - Starting epoch 0
W1008 13:31:11.121000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:11.544000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:12.164000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE addmm(16384x3072, 16384x768, 768x3072)
  triton_mm_54 0.4577 ms 100.0%
  triton_mm_47 0.4721 ms 97.0%
  triton_mm_51 0.4813 ms 95.1%
  triton_mm_49 0.4854 ms 94.3%
  triton_mm_48 0.4864 ms 94.1%
  triton_mm_52 0.4915 ms 93.1%
  triton_mm_45 0.4994 ms 91.7%
  bias_addmm 0.5081 ms 90.1%
  triton_mm_44 0.5140 ms 89.1%
  triton_mm_55 0.5284 ms 86.6%
SingleProcess AUTOTUNE benchmarking takes 2.3861 seconds and 0.0015 seconds precompiling
W1008 13:31:14.522000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:14.896000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:15.504000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE addmm(16384x2304, 16384x768, 768x2304)
  triton_mm_16 0.3492 ms 100.0%
  triton_mm_9 0.3603 ms 96.9%
  bias_addmm 0.3643 ms 95.8%
  triton_mm_13 0.3666 ms 95.3%
  triton_mm_10 0.3678 ms 94.9%
  triton_mm_11 0.3687 ms 94.7%
  triton_mm_14 0.3728 ms 93.7%
  triton_mm_7 0.3768 ms 92.7%
  triton_mm_6 0.3881 ms 90.0%
  triton_mm_17 0.4045 ms 86.3%
SingleProcess AUTOTUNE benchmarking takes 2.2256 seconds and 0.0012 seconds precompiling
W1008 13:31:16.592000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:16.932000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:17.497000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x768, 768x768)
  triton_mm_35 0.1219 ms 100.0%
  triton_mm_32 0.1241 ms 98.2%
  triton_mm_30 0.1260 ms 96.7%
  mm 0.1287 ms 94.7%
  triton_mm_29 0.1300 ms 93.7%
  triton_mm_36 0.1300 ms 93.7%
  triton_mm_26 0.1329 ms 91.7%
  triton_mm_33 0.1362 ms 89.5%
  triton_mm_28 0.1362 ms 89.5%
  triton_mm_25 0.1433 ms 85.0%
SingleProcess AUTOTUNE benchmarking takes 1.9914 seconds and 0.0014 seconds precompiling
W1008 13:31:18.658000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:19.029000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:19.645000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x3072, 3072x768)
  triton_mm_73 0.4536 ms 100.0%
  triton_mm_68 0.4577 ms 99.1%
  triton_mm_70 0.4598 ms 98.7%
  triton_mm_66 0.4611 ms 98.4%
  triton_mm_67 0.4623 ms 98.1%
  triton_mm_71 0.4628 ms 98.0%
  mm 0.4792 ms 94.7%
  triton_mm_74 0.4884 ms 92.9%
  triton_mm_64 0.5048 ms 89.9%
  triton_mm_72 0.5448 ms 83.3%
SingleProcess AUTOTUNE benchmarking takes 2.1458 seconds and 0.0020 seconds precompiling
W1008 13:31:21.504000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:22.003000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:22.838000 125782588630208 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x768, 768x50304)
  mm 7.8408 ms 100.0%
  triton_mm_925 7.8899 ms 99.4%
  triton_mm_926 8.1725 ms 95.9%
  triton_mm_923 8.1816 ms 95.8%
  triton_mm_928 8.3159 ms 94.3%
  triton_mm_922 8.5924 ms 91.3%
  triton_mm_929 8.7245 ms 89.9%
  triton_mm_921 8.8150 ms 88.9%
  triton_mm_918 8.8599 ms 88.5%
  triton_mm_919 8.9375 ms 87.7%
SingleProcess AUTOTUNE benchmarking takes 3.1885 seconds and 0.0017 seconds precompiling
W1008 13:31:51.387000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:51.762000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:52.383000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x768, 768x3072)
  triton_mm_978 0.4549 ms 100.0%
  triton_mm_982 0.4565 ms 99.6%
  triton_mm_985 0.4577 ms 99.4%
  triton_mm_980 0.4672 ms 97.4%
  triton_mm_983 0.4792 ms 94.9%
  triton_mm_979 0.4833 ms 94.1%
  triton_mm_976 0.4966 ms 91.6%
  mm 0.5007 ms 90.9%
  triton_mm_975 0.5079 ms 89.6%
  triton_mm_986 0.5079 ms 89.6%
SingleProcess AUTOTUNE benchmarking takes 2.1065 seconds and 0.0013 seconds precompiling
W1008 13:31:54.979000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:55.483000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:56.311000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(50304x16384, 16384x768)
  triton_mm_945 7.8449 ms 100.0%
  triton_mm_944 7.8603 ms 99.8%
  triton_mm_942 7.9222 ms 99.0%
  triton_mm_941 7.9882 ms 98.2%
  triton_mm_948 8.2883 ms 94.7%
  triton_mm_940 8.3476 ms 94.0%
  triton_mm_938 8.3845 ms 93.6%
  triton_mm_947 8.4664 ms 92.7%
  triton_mm_937 8.4726 ms 92.6%
  triton_mm_936 8.7080 ms 90.1%
SingleProcess AUTOTUNE benchmarking takes 2.9920 seconds and 0.0009 seconds precompiling
W1008 13:31:58.255000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:58.635000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:31:59.274000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(768x16384, 16384x3072)
  mm 0.5159 ms 100.0%
  triton_mm_994 0.5416 ms 95.3%
  triton_mm_995 0.5540 ms 93.1%
  triton_mm_993 0.5621 ms 91.8%
  triton_mm_1001 0.5939 ms 86.9%
  triton_mm_997 0.5950 ms 86.7%
  triton_mm_1002 0.5980 ms 86.3%
  triton_mm_998 0.6011 ms 85.8%
  triton_mm_999 0.6099 ms 84.6%
  triton_mm_988 0.6308 ms 81.8%
SingleProcess AUTOTUNE benchmarking takes 2.1454 seconds and 0.0012 seconds precompiling
W1008 13:32:00.821000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:01.202000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:01.856000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(3072x16384, 16384x768)
  mm 0.5233 ms 100.0%
  triton_mm_1032 0.5420 ms 96.5%
  triton_mm_1033 0.5450 ms 96.0%
  triton_mm_1031 0.5560 ms 94.1%
  triton_mm_1035 0.5949 ms 88.0%
  triton_mm_1039 0.5949 ms 88.0%
  triton_mm_1036 0.5990 ms 87.4%
  triton_mm_1040 0.6001 ms 87.2%
  triton_mm_1037 0.6101 ms 85.8%
  triton_mm_1026 0.6486 ms 80.7%
SingleProcess AUTOTUNE benchmarking takes 2.2173 seconds and 0.0019 seconds precompiling
W1008 13:32:03.367000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:03.739000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:04.351000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(2304x16384, 16384x768)
  mm 0.3881 ms 100.0%
  triton_mm_1119 0.3994 ms 97.2%
  triton_mm_1111 0.4014 ms 96.7%
  triton_mm_1115 0.4014 ms 96.7%
  triton_mm_1116 0.4014 ms 96.7%
  triton_mm_1112 0.4015 ms 96.7%
  triton_mm_1118 0.4024 ms 96.4%
  triton_mm_1113 0.4116 ms 94.3%
  triton_mm_1108 0.4407 ms 88.1%
  triton_mm_1107 0.4936 ms 78.6%
SingleProcess AUTOTUNE benchmarking takes 2.1524 seconds and 0.0019 seconds precompiling
W1008 13:32:06.376000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:06.734000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:07.334000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(768x16384, 16384x768)
  mm 0.1391 ms 100.0%
  triton_mm_1074 0.2068 ms 67.2%
  triton_mm_1078 0.2068 ms 67.2%
  triton_mm_1073 0.2130 ms 65.3%
  triton_mm_1077 0.2130 ms 65.3%
  triton_mm_1071 0.2161 ms 64.4%
  triton_mm_1075 0.2273 ms 61.2%
  triton_mm_1068 0.2335 ms 59.6%
  triton_mm_1066 0.2345 ms 59.3%
  triton_mm_1067 0.2345 ms 59.3%
SingleProcess AUTOTUNE benchmarking takes 2.0298 seconds and 0.0014 seconds precompiling
W1008 13:32:09.532000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:10.019000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:10.847000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x50304, 50304x768)
  triton_mm_961 7.9538 ms 100.0%
  triton_mm_964 8.0497 ms 98.8%
  triton_mm_966 8.0937 ms 98.3%
  triton_mm_960 8.1675 ms 97.4%
  triton_mm_963 8.1910 ms 97.1%
  triton_mm_967 8.3089 ms 95.7%
  triton_mm_959 8.5221 ms 93.3%
  triton_mm_957 8.8596 ms 89.8%
  triton_mm_956 9.0012 ms 88.4%
  triton_mm_965 9.1873 ms 86.6%
SingleProcess AUTOTUNE benchmarking takes 3.0821 seconds and 0.0013 seconds precompiling
W1008 13:32:11.967000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:12.330000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:12.947000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x3072, 3072x768)
  triton_mm_1023 0.4498 ms 100.0%
  triton_mm_1016 0.4526 ms 99.4%
  triton_mm_1020 0.4565 ms 98.5%
  triton_mm_1017 0.4575 ms 98.3%
  triton_mm_1018 0.4608 ms 97.6%
  triton_mm_1021 0.4649 ms 96.8%
  triton_mm_1024 0.4997 ms 90.0%
  triton_mm_1014 0.5059 ms 88.9%
  mm 0.5212 ms 86.3%
  triton_mm_1013 0.5271 ms 85.3%
SingleProcess AUTOTUNE benchmarking takes 2.0920 seconds and 0.0014 seconds precompiling
W1008 13:32:14.015000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:14.360000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:14.930000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x768, 768x768)
  triton_mm_1054 0.1208 ms 100.0%
  triton_mm_1061 0.1208 ms 100.0%
  triton_mm_1058 0.1219 ms 99.2%
  triton_mm_1056 0.1256 ms 96.2%
  mm 0.1257 ms 96.1%
  triton_mm_1055 0.1300 ms 92.9%
  triton_mm_1062 0.1332 ms 90.7%
  triton_mm_1059 0.1333 ms 90.6%
  triton_mm_1052 0.1339 ms 90.2%
  triton_mm_1051 0.1427 ms 84.7%
SingleProcess AUTOTUNE benchmarking takes 1.9815 seconds and 0.0011 seconds precompiling
W1008 13:32:16.048000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:16.408000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
W1008 13:32:17.005000 125778809325120 torch/_inductor/select_algorithm.py:1469] [1/0_1] out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.
AUTOTUNE mm(16384x2304, 2304x768)
  triton_mm_1099 0.3410 ms 100.0%
  triton_mm_1092 0.3430 ms 99.4%
  triton_mm_1096 0.3442 ms 99.1%
  triton_mm_1093 0.3451 ms 98.8%
  triton_mm_1094 0.3492 ms 97.7%
  triton_mm_1097 0.3512 ms 97.1%
  triton_mm_1100 0.3809 ms 89.5%
  triton_mm_1090 0.3829 ms 89.1%
  triton_mm_1089 0.3870 ms 88.1%
  mm 0.3922 ms 86.9%
SingleProcess AUTOTUNE benchmarking takes 2.0740 seconds and 0.0013 seconds precompiling
W1008 13:32:27.657000 125782588630208 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-08 13:32:27,849 - INFO - tokens per second = 5441 | loss: 10.977471 | gradient norm 0.1897 | dt 96364.574909 ms | CUDA memory: 526.96 MB | CPU memory: 4.44 GB
2024-10-08 13:32:32,206 - INFO - tokens per second = 120439 | loss: 10.962574 | gradient norm 0.2037 | dt 4353.133917 ms | CUDA memory: 1478.46 MB | CPU memory: 4.50 GB
2024-10-08 13:32:36,292 - INFO - tokens per second = 128329 | loss: 10.919746 | gradient norm 0.1842 | dt 4085.504293 ms | CUDA memory: 1478.46 MB | CPU memory: 4.50 GB
2024-10-08 13:32:40,487 - INFO - tokens per second = 124964 | loss: 10.864514 | gradient norm 0.1847 | dt 4195.505142 ms | CUDA memory: 1478.46 MB | CPU memory: 4.50 GB
2024-10-08 13:32:44,582 - INFO - tokens per second = 128040 | loss: 10.790728 | gradient norm 0.1888 | dt 4094.729185 ms | CUDA memory: 1478.46 MB | CPU memory: 4.50 GB
2024-10-08 13:32:48,687 - INFO - tokens per second = 127714 | loss: 10.698334 | gradient norm 0.1813 | dt 4105.183125 ms | CUDA memory: 1478.46 MB | CPU memory: 4.50 GB
2024-10-08 13:32:52,783 - INFO - tokens per second = 128002 | loss: 10.607010 | gradient norm 0.1611 | dt 4095.940351 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:32:56,881 - INFO - tokens per second = 127938 | loss: 10.516893 | gradient norm 0.1438 | dt 4097.990274 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:01,022 - INFO - tokens per second = 126603 | loss: 10.424747 | gradient norm 0.1244 | dt 4141.203642 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:05,123 - INFO - tokens per second = 127854 | loss: 10.343302 | gradient norm 0.1197 | dt 4100.674391 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:09,226 - INFO - tokens per second = 127791 | loss: 10.261707 | gradient norm 0.1093 | dt 4102.701426 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:13,378 - INFO - tokens per second = 126270 | loss: 10.205348 | gradient norm 0.1004 | dt 4152.116060 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:17,526 - INFO - tokens per second = 126377 | loss: 10.144988 | gradient norm 0.1011 | dt 4148.598909 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:21,623 - INFO - tokens per second = 127968 | loss: 10.091425 | gradient norm 0.0955 | dt 4097.022057 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:25,734 - INFO - tokens per second = 127540 | loss: 10.011429 | gradient norm 0.0924 | dt 4110.772610 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:29,844 - INFO - tokens per second = 127566 | loss: 9.962547 | gradient norm 0.0889 | dt 4109.940052 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:33,958 - INFO - tokens per second = 127456 | loss: 9.921144 | gradient norm 0.1059 | dt 4113.472939 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:38,060 - INFO - tokens per second = 127792 | loss: 9.873049 | gradient norm 0.0795 | dt 4102.679014 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:42,212 - INFO - tokens per second = 126279 | loss: 9.879757 | gradient norm 0.0838 | dt 4151.809692 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:46,317 - INFO - tokens per second = 127732 | loss: 9.863353 | gradient norm 0.0743 | dt 4104.595184 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:50,431 - INFO - tokens per second = 127446 | loss: 9.757661 | gradient norm 0.0721 | dt 4113.794327 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:54,532 - INFO - tokens per second = 127820 | loss: 9.707099 | gradient norm 0.1315 | dt 4101.782322 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:33:58,647 - INFO - tokens per second = 127407 | loss: 9.693260 | gradient norm 0.0711 | dt 4115.054607 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:02,750 - INFO - tokens per second = 127803 | loss: 9.679356 | gradient norm 0.0674 | dt 4102.315187 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:06,857 - INFO - tokens per second = 127631 | loss: 9.630776 | gradient norm 0.0669 | dt 4107.827425 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:10,971 - INFO - tokens per second = 127453 | loss: 9.619995 | gradient norm 0.0675 | dt 4113.587618 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:15,081 - INFO - tokens per second = 127578 | loss: 9.601505 | gradient norm 0.0693 | dt 4109.542847 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:19,191 - INFO - tokens per second = 127550 | loss: 9.568857 | gradient norm 0.0713 | dt 4110.435009 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:23,352 - INFO - tokens per second = 126005 | loss: 9.549366 | gradient norm 0.0668 | dt 4160.838366 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:27,471 - INFO - tokens per second = 127283 | loss: 9.510444 | gradient norm 0.0688 | dt 4119.057655 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:31,587 - INFO - tokens per second = 127373 | loss: 9.537268 | gradient norm 0.0660 | dt 4116.167784 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:35,750 - INFO - tokens per second = 125957 | loss: 9.484021 | gradient norm 0.0649 | dt 4162.422895 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:39,896 - INFO - tokens per second = 126450 | loss: 9.483685 | gradient norm 0.0644 | dt 4146.210194 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:44,102 - INFO - tokens per second = 124640 | loss: 9.447267 | gradient norm 0.0678 | dt 4206.423521 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:48,226 - INFO - tokens per second = 127125 | loss: 9.378181 | gradient norm 0.0650 | dt 4124.187708 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:52,342 - INFO - tokens per second = 127393 | loss: 9.409286 | gradient norm 0.0670 | dt 4115.525246 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:34:56,459 - INFO - tokens per second = 127337 | loss: 9.401505 | gradient norm 0.0627 | dt 4117.330313 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:00,582 - INFO - tokens per second = 127174 | loss: 9.349997 | gradient norm 0.0695 | dt 4122.597218 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:04,697 - INFO - tokens per second = 127403 | loss: 9.329565 | gradient norm 0.0630 | dt 4115.202427 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:08,813 - INFO - tokens per second = 127368 | loss: 9.328656 | gradient norm 0.0685 | dt 4116.316557 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:12,928 - INFO - tokens per second = 127422 | loss: 9.309172 | gradient norm 0.0639 | dt 4114.590406 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:17,096 - INFO - tokens per second = 125798 | loss: 9.241007 | gradient norm 0.0748 | dt 4167.702675 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:21,319 - INFO - tokens per second = 124127 | loss: 9.233432 | gradient norm 0.0665 | dt 4223.791361 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:25,447 - INFO - tokens per second = 127021 | loss: 9.212839 | gradient norm 0.0631 | dt 4127.553463 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:29,575 - INFO - tokens per second = 127010 | loss: 9.207184 | gradient norm 0.0608 | dt 4127.925873 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:33,697 - INFO - tokens per second = 127206 | loss: 9.195283 | gradient norm 0.0630 | dt 4121.580362 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:37,821 - INFO - tokens per second = 127116 | loss: 9.148927 | gradient norm 0.0676 | dt 4124.470949 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:41,937 - INFO - tokens per second = 127380 | loss: 9.117874 | gradient norm 0.0578 | dt 4115.938663 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:46,101 - INFO - tokens per second = 125914 | loss: 9.101945 | gradient norm 0.0601 | dt 4163.873196 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:50,229 - INFO - tokens per second = 126988 | loss: 9.045558 | gradient norm 0.0606 | dt 4128.626823 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:54,346 - INFO - tokens per second = 127375 | loss: 9.054341 | gradient norm 0.0622 | dt 4116.111517 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:35:58,516 - INFO - tokens per second = 125702 | loss: 9.014632 | gradient norm 0.0627 | dt 4170.873880 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:02,633 - INFO - tokens per second = 127362 | loss: 8.988431 | gradient norm 0.0593 | dt 4116.503477 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:06,795 - INFO - tokens per second = 125973 | loss: 8.953780 | gradient norm 0.0571 | dt 4161.903381 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:10,964 - INFO - tokens per second = 125745 | loss: 8.949102 | gradient norm 0.0607 | dt 4169.463396 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:15,081 - INFO - tokens per second = 127356 | loss: 8.913946 | gradient norm 0.0550 | dt 4116.719723 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:19,194 - INFO - tokens per second = 127458 | loss: 8.881281 | gradient norm 0.0518 | dt 4113.431215 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:23,314 - INFO - tokens per second = 127271 | loss: 8.865036 | gradient norm 0.0520 | dt 4119.476557 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:27,436 - INFO - tokens per second = 127196 | loss: 8.833995 | gradient norm 0.0540 | dt 4121.884584 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:31,557 - INFO - tokens per second = 127218 | loss: 8.805489 | gradient norm 0.0590 | dt 4121.177912 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:35,694 - INFO - tokens per second = 126737 | loss: 8.785865 | gradient norm 0.0537 | dt 4136.805534 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:39,860 - INFO - tokens per second = 125865 | loss: 8.771263 | gradient norm 0.0567 | dt 4165.495157 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:44,030 - INFO - tokens per second = 125705 | loss: 8.702581 | gradient norm 0.0511 | dt 4170.794725 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:48,248 - INFO - tokens per second = 124296 | loss: 8.671979 | gradient norm 0.0481 | dt 4218.046665 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:52,484 - INFO - tokens per second = 123773 | loss: 8.634469 | gradient norm 0.0507 | dt 4235.883951 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:36:56,743 - INFO - tokens per second = 123108 | loss: 8.617800 | gradient norm 0.0536 | dt 4258.757591 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:00,923 - INFO - tokens per second = 125428 | loss: 8.573665 | gradient norm 0.0527 | dt 4180.007935 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:05,061 - INFO - tokens per second = 126708 | loss: 8.562458 | gradient norm 0.0492 | dt 4137.768030 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:09,199 - INFO - tokens per second = 126677 | loss: 8.539413 | gradient norm 0.0542 | dt 4138.770342 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:13,334 - INFO - tokens per second = 126800 | loss: 8.461497 | gradient norm 0.0568 | dt 4134.751558 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:17,601 - INFO - tokens per second = 122883 | loss: 8.434731 | gradient norm 0.0495 | dt 4266.546488 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:21,772 - INFO - tokens per second = 125688 | loss: 8.432494 | gradient norm 0.0454 | dt 4171.348572 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:26,016 - INFO - tokens per second = 123547 | loss: 8.367242 | gradient norm 0.0478 | dt 4243.646383 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:30,172 - INFO - tokens per second = 126145 | loss: 8.331259 | gradient norm 0.0507 | dt 4156.231165 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:34,349 - INFO - tokens per second = 125534 | loss: 8.298794 | gradient norm 0.0424 | dt 4176.476479 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:38,482 - INFO - tokens per second = 126848 | loss: 8.296256 | gradient norm 0.0422 | dt 4133.183956 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:42,727 - INFO - tokens per second = 123483 | loss: 8.239169 | gradient norm 0.0433 | dt 4245.844126 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:46,937 - INFO - tokens per second = 124542 | loss: 8.231810 | gradient norm 0.0419 | dt 4209.727526 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:51,052 - INFO - tokens per second = 127401 | loss: 8.197954 | gradient norm 0.0448 | dt 4115.248203 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:55,171 - INFO - tokens per second = 127282 | loss: 8.152917 | gradient norm 0.0420 | dt 4119.097948 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:37:59,340 - INFO - tokens per second = 125783 | loss: 8.119519 | gradient norm 0.0380 | dt 4168.195009 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:03,462 - INFO - tokens per second = 127189 | loss: 8.106076 | gradient norm 0.0384 | dt 4122.121096 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:07,584 - INFO - tokens per second = 127200 | loss: 8.055618 | gradient norm 0.0390 | dt 4121.747255 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:11,705 - INFO - tokens per second = 127216 | loss: 8.008725 | gradient norm 0.0374 | dt 4121.258259 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:15,868 - INFO - tokens per second = 125916 | loss: 7.992314 | gradient norm 0.0374 | dt 4163.778543 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:19,992 - INFO - tokens per second = 127133 | loss: 7.945232 | gradient norm 0.0343 | dt 4123.925447 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:24,168 - INFO - tokens per second = 125559 | loss: 7.929183 | gradient norm 0.0438 | dt 4175.643206 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:28,294 - INFO - tokens per second = 127074 | loss: 7.910954 | gradient norm 0.0519 | dt 4125.863075 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:32,414 - INFO - tokens per second = 127249 | loss: 7.887442 | gradient norm 0.0420 | dt 4120.157719 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:36,683 - INFO - tokens per second = 122811 | loss: 7.912468 | gradient norm 0.0475 | dt 4269.054413 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:40,870 - INFO - tokens per second = 125221 | loss: 7.823141 | gradient norm 0.0420 | dt 4186.885595 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
2024-10-08 13:38:45,123 - INFO - tokens per second = 123267 | loss: 7.832599 | gradient norm 0.0605 | dt 4253.256083 ms | CUDA memory: 1478.46 MB | CPU memory: 4.51 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 303, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 266, in training_step
    @torch.compile(mode="max-autotune", disable=disable_compilation)
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 268, in torch_dynamo_resume_in_training_step_at_268
    optimizer.zero_grad()
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
