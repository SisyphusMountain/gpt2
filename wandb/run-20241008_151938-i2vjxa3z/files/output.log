2024-10-08 15:19:39,632 - WARNING - Disabling compilation
2024-10-08 15:19:40,604 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-08 15:19:41,179 - INFO - total desired batch size: 524288
2024-10-08 15:19:41,179 - INFO - => computed gradient accumulation steps 32
2024-10-08 15:19:46,222 - INFO - tokens per second = 104023 | loss: 10.623211 | gradient norm 0.1650 | dt 5040.137768 ms | CUDA memory: 1022.55 MB | CPU memory: 1.98 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 294, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 265, in training_step
    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 21, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 82, in clip_grad_norm_
    clip_coef = max_norm / (total_norm + 1e-6)
                ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_tensor.py", line 35, in wrapped
    @functools.wraps(f, assigned=assigned)

KeyboardInterrupt
