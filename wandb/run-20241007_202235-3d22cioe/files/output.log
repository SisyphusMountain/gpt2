2024-10-07 20:22:36,419 - INFO - Enabling compilation
2024-10-07 20:22:37,695 - INFO - used cuda memory after creating model: 552298496
Number of decayed parameter tensors 50 for a total of 124354560 parameters
Number of non-decayed parameter tensors 98 for a total of 121344 parameters
2024-10-07 20:22:38,534 - INFO - total desired batch size: 524288
2024-10-07 20:22:38,535 - INFO - => computed gradient accumulation steps 32
2024-10-07 20:22:38,535 - INFO - Starting epoch 0
W1007 20:23:01.577000 130171940984000 torch/_logging/_internal.py:1034] [6/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
2024-10-07 20:23:01,769 - INFO - tokens per second = 22565 | loss: 11.005906 | gradient norm 0.1550 | dt 23234.099865 ms | CUDA memory: 526.96 MB | CPU memory: 1.93 GB
2024-10-07 20:23:06,047 - INFO - tokens per second = 122724 | loss: 10.990939 | gradient norm 0.1729 | dt 4272.081852 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:10,329 - INFO - tokens per second = 122419 | loss: 10.956208 | gradient norm 0.1512 | dt 4282.719374 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:14,646 - INFO - tokens per second = 121441 | loss: 10.908748 | gradient norm 0.1540 | dt 4317.206621 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:18,796 - INFO - tokens per second = 126331 | loss: 10.849716 | gradient norm 0.1575 | dt 4150.117636 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:22,863 - INFO - tokens per second = 128901 | loss: 10.777087 | gradient norm 0.1626 | dt 4067.366838 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:26,993 - INFO - tokens per second = 126992 | loss: 10.695230 | gradient norm 0.1415 | dt 4128.524780 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:31,190 - INFO - tokens per second = 124887 | loss: 10.606620 | gradient norm 0.1354 | dt 4198.101044 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:35,522 - INFO - tokens per second = 121031 | loss: 10.518190 | gradient norm 0.1201 | dt 4331.852198 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:39,739 - INFO - tokens per second = 124340 | loss: 10.441563 | gradient norm 0.1178 | dt 4216.569424 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:43,921 - INFO - tokens per second = 125355 | loss: 10.361733 | gradient norm 0.1089 | dt 4182.419062 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:48,054 - INFO - tokens per second = 126846 | loss: 10.299778 | gradient norm 0.0980 | dt 4133.251190 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:52,327 - INFO - tokens per second = 122712 | loss: 10.236456 | gradient norm 0.0996 | dt 4272.505283 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:23:56,499 - INFO - tokens per second = 125686 | loss: 10.180896 | gradient norm 0.0955 | dt 4171.423197 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:00,690 - INFO - tokens per second = 125077 | loss: 10.104372 | gradient norm 0.0946 | dt 4191.715240 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:04,906 - INFO - tokens per second = 124357 | loss: 10.057158 | gradient norm 0.0894 | dt 4215.980530 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:09,212 - INFO - tokens per second = 121756 | loss: 10.004834 | gradient norm 0.1091 | dt 4306.070566 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:13,543 - INFO - tokens per second = 121046 | loss: 9.955763 | gradient norm 0.0825 | dt 4331.305265 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:18,099 - INFO - tokens per second = 115087 | loss: 9.954992 | gradient norm 0.0852 | dt 4555.584431 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:22,645 - INFO - tokens per second = 115325 | loss: 9.927395 | gradient norm 0.0780 | dt 4546.185255 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:27,146 - INFO - tokens per second = 116490 | loss: 9.832750 | gradient norm 0.0748 | dt 4500.730038 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:31,576 - INFO - tokens per second = 118345 | loss: 9.784699 | gradient norm 0.1345 | dt 4430.164576 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:36,140 - INFO - tokens per second = 114886 | loss: 9.773027 | gradient norm 0.0730 | dt 4563.560009 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:40,697 - INFO - tokens per second = 115040 | loss: 9.738857 | gradient norm 0.0687 | dt 4557.437181 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:45,230 - INFO - tokens per second = 115672 | loss: 9.699693 | gradient norm 0.0684 | dt 4532.530308 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:49,772 - INFO - tokens per second = 115416 | loss: 9.683929 | gradient norm 0.0693 | dt 4542.599440 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:54,323 - INFO - tokens per second = 115205 | loss: 9.661320 | gradient norm 0.0703 | dt 4550.907850 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:24:58,872 - INFO - tokens per second = 115247 | loss: 9.628628 | gradient norm 0.0734 | dt 4549.249649 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:25:03,424 - INFO - tokens per second = 115192 | loss: 9.605956 | gradient norm 0.0683 | dt 4551.444769 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:25:07,986 - INFO - tokens per second = 114916 | loss: 9.574301 | gradient norm 0.0707 | dt 4562.341690 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:25:12,402 - INFO - tokens per second = 118713 | loss: 9.595835 | gradient norm 0.0682 | dt 4416.440487 ms | CUDA memory: 1478.46 MB | CPU memory: 1.99 GB
2024-10-07 20:25:16,532 - INFO - tokens per second = 126964 | loss: 9.544930 | gradient norm 0.0670 | dt 4129.421234 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:20,729 - INFO - tokens per second = 124931 | loss: 9.539044 | gradient norm 0.0664 | dt 4196.627378 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:24,908 - INFO - tokens per second = 125425 | loss: 9.504704 | gradient norm 0.0700 | dt 4180.082083 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:29,057 - INFO - tokens per second = 126385 | loss: 9.435202 | gradient norm 0.0662 | dt 4148.340702 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:33,221 - INFO - tokens per second = 125910 | loss: 9.464340 | gradient norm 0.0696 | dt 4163.996458 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:37,310 - INFO - tokens per second = 128202 | loss: 9.450798 | gradient norm 0.0642 | dt 4089.532375 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:41,425 - INFO - tokens per second = 127426 | loss: 9.403790 | gradient norm 0.0708 | dt 4114.458323 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:45,500 - INFO - tokens per second = 128663 | loss: 9.380904 | gradient norm 0.0644 | dt 4074.899197 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:49,573 - INFO - tokens per second = 128710 | loss: 9.381887 | gradient norm 0.0698 | dt 4073.413134 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:53,647 - INFO - tokens per second = 128694 | loss: 9.363866 | gradient norm 0.0654 | dt 4073.908567 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:25:57,783 - INFO - tokens per second = 126754 | loss: 9.292432 | gradient norm 0.0761 | dt 4136.248827 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:26:01,941 - INFO - tokens per second = 126101 | loss: 9.284417 | gradient norm 0.0679 | dt 4157.673359 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
2024-10-07 20:26:06,193 - INFO - tokens per second = 123310 | loss: 9.265997 | gradient norm 0.0650 | dt 4251.785755 ms | CUDA memory: 1478.46 MB | CPU memory: 2.00 GB
Traceback (most recent call last):
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 302, in <module>
    loss, norm = training_step(x, y)
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 433, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 265, in training_step
    @torch.compile(mode="max-autotune", disable=disable_compilation)
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 267, in torch_dynamo_resume_in_training_step_at_267
    optimizer.zero_grad()
  File "/home/enzo/Documents/gpt2/gpt2_fabric.py", line 271, in torch_dynamo_resume_in_training_step_at_271
    loss.backward()
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 21, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py", line 82, in clip_grad_norm_
    clip_coef = max_norm / (total_norm + 1e-6)
                ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
  File "/home/enzo/mambaforge/envs/up_to_date/lib/python3.12/site-packages/torch/_tensor.py", line 35, in wrapped
    @functools.wraps(f, assigned=assigned)

KeyboardInterrupt
